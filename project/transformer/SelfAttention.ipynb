{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import color\n",
    "from color import magenta, green, red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        \n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads \n",
    "        \n",
    "        assert (self.head_dim * heads == embed_size), \"Embed size needs to be divisible by heads\"\n",
    "        \n",
    "        # Explanation: Ensures that 'embed_size' is divisible by 'heads'. This is because the embedding is cut up into chunks and fed into identical but seperate attention heads. \n",
    "        # Each head sees a reduced dimension of the embedding which is concatonated at the end to form the final full form. This was better than just one single headed attention\n",
    "        # according to the \"Attention is all you need\" paper.\n",
    "        \n",
    "        self.query_weights = nn.Linear(self.head_dim, self.head_dim, bias=False) # The query needs to be head_dim x head_dim because it is multiplied by the key which is head_dim x head_dim\n",
    "        self.key_weights = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.value_weights = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        \n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "    \n",
    "    def forward(self, queries, keys, values, mask, testing_mode=False): # Actual Queries, Keys and Values are passed in here, not the same as weight matrices\n",
    "        \n",
    "        # queries, keys, values have shape: (num_examples, seq_length, embed_size)\n",
    "\n",
    "        num_examples = queries.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], queries.shape[1]\n",
    "        \n",
    "        # Split embedding into self.heads pieces\n",
    "        # queries, keys, values have a new shape: (num examples, seq length, num heads, head dimension)\n",
    "        \n",
    "        queries = queries.reshape(num_examples, query_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(num_examples, key_len, self.heads, self.head_dim)\n",
    "        values = values.reshape(num_examples, value_len, self.heads, self.head_dim)\n",
    "        \n",
    "        queries = self.query_weights(queries)\n",
    "        keys = self.key_weights(keys)\n",
    "        values = self.value_weights(values)\n",
    "        \n",
    "        # Size should be: [batch size, seq length, num heads, head dimension]\n",
    "        if testing_mode:\n",
    "            print(magenta('Testing Self Attention:')) \n",
    "            if queries.shape[0] == num_examples and queries.shape[1] == query_len and queries.shape[2] == self.heads and queries.shape[3] == self.head_dim: \n",
    "                print('Size of query is', green('correct'))\n",
    "            else:\n",
    "                print('Size of query is', red('incorrect'))\n",
    "                print(queries.shape, red('does not match'), [num_examples, query_len, self.heads, self.head_dim])\n",
    "        \n",
    "        # Matmul Q and K\n",
    "        # queries_dot_values shape: (num examples, num heads, query_len, key_len)\n",
    "        # nqhd, nkhd -> nhqk (n: number of examples, q: query length, k: key length, h: number of heads, d: head dimension)\n",
    "        \n",
    "        queries_dot_values = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])                                           # einsum is a more efficient way of doing matrix multiplication\n",
    "        queries_dot_values = queries_dot_values / np.sqrt(self.head_dim)\n",
    "        \n",
    "        if mask is not None:\n",
    "            queries_dot_values = queries_dot_values.masked_fill(mask == 0, float(\"-1e20\"))                              # Masking out the padded values, use -1e20 because softmax will make it close to 0\n",
    "        \n",
    "        attention = torch.softmax(queries_dot_values, dim=-1)                                                           # dim=-1 means the last dimension\n",
    "        out = torch.einsum(\"nhqk, nlhd -> nqhd\", [attention, values]).reshape(num_examples, query_len, self.embed_size) # multiply attention by values and reshape to original shape with embed length\n",
    "        \n",
    "        # Size should be:[num examples, seq length, embed size]\n",
    "        if testing_mode: \n",
    "            if out.shape[0] == num_examples and out.shape[1] == value_len and out.shape[2] == self.embed_size: \n",
    "                print('Size of output is', green('correct'))\n",
    "            else:\n",
    "                print('Size of output is', red('incorrect'))\n",
    "                print(out.shape, red('does not match'), [num_examples, value_len, self.embed_size])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[10;10;35mTesting Self Attention:\u001b[0m\n",
      "Size of query is \u001b[10;10;32mcorrect\u001b[0m\n",
      "Size of output is \u001b[10;10;32mcorrect\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# TESTING SELF ATTENTION LAYERS\n",
    "\n",
    "embed_size = 512\n",
    "heads = 8\n",
    "seq_length = 10\n",
    "batch_size = 4\n",
    "\n",
    "queries = torch.rand(batch_size, seq_length, embed_size)\n",
    "keys = torch.rand(batch_size, seq_length, embed_size)\n",
    "values = torch.rand(batch_size, seq_length, embed_size)\n",
    "\n",
    "self_attention = SelfAttention(embed_size, heads)\n",
    "self_attention.forward(queries, keys, values, None, testing_mode=True)\n",
    "\n",
    "# Output should be of shape [batch size, seq length, num heads, head dimension]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear layer\n",
      " Parameter containing:\n",
      "tensor([[ 0.3166,  0.4056,  0.3759, -0.2346,  0.0049],\n",
      "        [ 0.0269,  0.0996, -0.3733,  0.0618,  0.2073],\n",
      "        [ 0.2639, -0.0843, -0.3288,  0.2310, -0.3859],\n",
      "        [ 0.2381, -0.2076, -0.1744, -0.3358, -0.2447]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "linear layer bias\n",
      " None\n",
      "\n",
      "weight matrix\n",
      " tensor([[ 0.,  1.,  2.,  3.,  4.],\n",
      "        [ 5.,  6.,  7.,  8.,  9.],\n",
      "        [10., 11., 12., 13., 14.],\n",
      "        [15., 16., 17., 18., 19.]], dtype=torch.float64)\n",
      "new_layer layer\n",
      " tensor([[  0.4730,   0.3675,  -1.5926,  -2.5428],\n",
      "        [  4.8147,   0.4786,  -3.1133,  -6.1650],\n",
      "        [  9.1564,   0.5898,  -4.6340,  -9.7872],\n",
      "        [ 13.4982,   0.7009,  -6.1547, -13.4095]], dtype=torch.float64,\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Experimenting with linear layers\n",
    "\n",
    "linear_layer = nn.Linear(5, 4, bias=False, dtype=float)\n",
    "weights = torch.arange(20, dtype=float).reshape(4, 5)\n",
    "\n",
    "new_layer = linear_layer(weights)\n",
    "\n",
    "print('linear layer\\n', linear_layer.weight)\n",
    "print('linear layer bias\\n',linear_layer.bias) # should be none\n",
    "\n",
    "print('\\nweight matrix\\n',weights)\n",
    "print('new_layer layer\\n' ,new_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[10;10;35moriginal tensor\n",
      "\u001b[0m tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "\u001b[10;10;35m\n",
      "3 partitions of tensor using.split:\n",
      "\u001b[0m tensor([0, 1, 2, 3]) \n",
      " tensor([4, 5, 6, 7]) \n",
      " tensor([ 8,  9, 10, 11])\n"
     ]
    }
   ],
   "source": [
    "# Figuring out .split()\n",
    "# .split takes arguments: split_size_or_sections, in this case we want to split the embedding into 3 parts\n",
    "arrange_1to12 = torch.arange(12)\n",
    "arrange_0to3, arrange_4to7, arrange_8to11 = arrange_1to12.split(4)[0], arrange_1to12.split(4)[1], arrange_1to12.split(4)[2]\n",
    "\n",
    "print(magenta('original tensor\\n'), arrange_1to12)\n",
    "print(magenta('\\n3 partitions of tensor using.split:\\n'),arrange_0to3,'\\n' ,arrange_4to7,'\\n' ,arrange_8to11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[10;10;35m10 sentences, of 5 words, each with a 10 integer embedding\n",
      "\u001b[0m\n",
      "torch.Size([10, 5, 10])\n",
      "\u001b[10;10;35m\n",
      "Reshaping into 5 heads with a dimensionality of 2 instead of 10\n",
      "\u001b[0m\n",
      "torch.Size([10, 5, 5, 2])\n"
     ]
    }
   ],
   "source": [
    "# Experimenting with torch reshape\n",
    "\n",
    "num_examples = 10\n",
    "seq_length = 5\n",
    "embed_size = 10\n",
    "heads = 5\n",
    "\n",
    "x = torch.arange(num_examples*seq_length*embed_size).reshape(num_examples, seq_length, embed_size)\n",
    "print(magenta('10 sentences, of 5 words, each with a 10 integer embedding\\n'))\n",
    "print(x.shape)\n",
    "\n",
    "print(magenta('\\nReshaping into 5 heads with a dimensionality of 2 instead of 10\\n'))\n",
    "\n",
    "x = x.reshape(num_examples, seq_length, heads, embed_size // heads)\n",
    "print(x.shape) # Numbers are arranged in a weird way, but does that really matter on intialization? probably not"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
