{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import color\n",
    "from color import magenta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        \n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads \n",
    "        \n",
    "        assert (self.head_dim * heads == embed_size), \"Embed size needs to be divisible by heads\"\n",
    "        \n",
    "        # Explanation: Ensures that 'embed_size' is divisible by 'heads'. This is because the embedding is cut up into chunks and fed into identical but seperate attention heads. \n",
    "        # Each head sees a reduced dimension of the embedding which is concatonated at the end to form the final full form. This was better than just one single headed attention\n",
    "        # according to the \"Attention is all you need\" paper.\n",
    "        \n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False) # The query needs to be head_dim x head_dim because it is multiplied by the key which is head_dim x head_dim\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        \n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "    \n",
    "    def forward(self, queries, keys, values, mask):\n",
    "        \n",
    "        # queries, keys, values have shape: (num_examples, seq_length, embed_size)\n",
    "        num_examples = queries.shape[0] # Number of examples in the batch\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], queries.shape[1]\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Split embedding into self.heads pieces\n",
    "        self.queries = queries.reshape(num_examples, query_len, self.heads, self.head_dim)\n",
    "        self.keys = keys.reshape(num_examples, key_len, self.heads, self.head_dim)\n",
    "        self.values = values.reshape(num_examples, value_len, self.heads, self.head_dim)\n",
    "        # queries, keys, values have a new shape: (num examples, seq length, num heads, head dimension)\n",
    "        \n",
    "        self.qk = torch.einsum(\"nqhd,nkhd->nhqk\", [self.queries, self.keys]) # (num examples, num heads, query len, key len)\n",
    "        print(self.qk.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[10;10;35mTesting self attention\u001b[0m\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot assign 'torch.FloatTensor' as child module 'queries' (torch.nn.Module or None expected)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m values \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(batch_size, seq_length, embed_size)\n\u001b[1;32m     15\u001b[0m self_attention \u001b[38;5;241m=\u001b[39m SelfAttention(embed_size, heads)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mself_attention\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# self_attention.forward(queries, keys, values, None)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 28\u001b[0m, in \u001b[0;36mSelfAttention.forward\u001b[0;34m(self, queries, keys, values, mask)\u001b[0m\n\u001b[1;32m     25\u001b[0m value_len, key_len, query_len \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], keys\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], queries\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Split embedding into self.heads pieces\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqueries\u001b[49m \u001b[38;5;241m=\u001b[39m queries\u001b[38;5;241m.\u001b[39mreshape(num_examples, query_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys \u001b[38;5;241m=\u001b[39m keys\u001b[38;5;241m.\u001b[39mreshape(num_examples, key_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mreshape(num_examples, value_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n",
      "File \u001b[0;32m~/dev/llm-experiments/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1733\u001b[0m, in \u001b[0;36mModule.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1731\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1732\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1733\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot assign \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mtypename(value)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as child module \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1734\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(torch.nn.Module or None expected)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1735\u001b[0m                         )\n\u001b[1;32m   1736\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m _global_module_registration_hooks\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m   1737\u001b[0m         output \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, name, value)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot assign 'torch.FloatTensor' as child module 'queries' (torch.nn.Module or None expected)"
     ]
    }
   ],
   "source": [
    "# Test what we have so far:\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(magenta(\"Testing self attention\"))\n",
    "    \n",
    "    embed_size = 512\n",
    "    heads = 8\n",
    "    seq_length = 10\n",
    "    batch_size = 4\n",
    "    \n",
    "    queries = torch.rand(batch_size, seq_length, embed_size)\n",
    "    keys = torch.rand(batch_size, seq_length, embed_size)\n",
    "    values = torch.rand(batch_size, seq_length, embed_size)\n",
    "    \n",
    "    self_attention = SelfAttention(embed_size, heads)\n",
    "    self_attention.forward(queries, keys, values, None)\n",
    "    # self_attention.forward(queries, keys, values, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear layer\n",
      " Parameter containing:\n",
      "tensor([[-0.3275,  0.2698,  0.1238, -0.3431, -0.3357],\n",
      "        [ 0.2703,  0.2390,  0.1315, -0.3263, -0.1824],\n",
      "        [ 0.1595, -0.3666,  0.3468, -0.3971,  0.2737],\n",
      "        [-0.2088, -0.0380,  0.0654, -0.2136, -0.1499]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "linear layer bias\n",
      " None\n",
      "\n",
      "weight matrix\n",
      " tensor([[ 0.,  1.,  2.,  3.,  4.],\n",
      "        [ 5.,  6.,  7.,  8.,  9.],\n",
      "        [10., 11., 12., 13., 14.],\n",
      "        [15., 16., 17., 18., 19.]], dtype=torch.float64)\n",
      "new_layer layer\n",
      " tensor([[ -1.8546,  -1.2065,   0.2308,  -1.1475],\n",
      "        [ -4.9180,  -0.5458,   0.3127,  -3.8720],\n",
      "        [ -7.9813,   0.1149,   0.3945,  -6.5964],\n",
      "        [-11.0447,   0.7755,   0.4764,  -9.3209]], dtype=torch.float64,\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Experimenting with linear layers\n",
    "\n",
    "linear_layer = nn.Linear(5, 4, bias=False, dtype=float)\n",
    "weights = torch.arange(20, dtype=float).reshape(4, 5)\n",
    "\n",
    "new_layer = linear_layer(weights)\n",
    "\n",
    "print('linear layer\\n', linear_layer.weight)\n",
    "print('linear layer bias\\n',linear_layer.bias) # should be none\n",
    "\n",
    "print('\\nweight matrix\\n',weights)\n",
    "print('new_layer layer\\n' ,new_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[10;10;35moriginal tensor\n",
      "\u001b[0m tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "\u001b[10;10;35m\n",
      "3 partitions of tensor using.split:\n",
      "\u001b[0m tensor([0, 1, 2, 3]) \n",
      " tensor([4, 5, 6, 7]) \n",
      " tensor([ 8,  9, 10, 11])\n"
     ]
    }
   ],
   "source": [
    "# Figuring out .split()\n",
    "# .split takes arguments: split_size_or_sections, in this case we want to split the embedding into 3 parts\n",
    "arrange_1to12 = torch.arange(12)\n",
    "arrange_0to3, arrange_4to7, arrange_8to11 = arrange_1to12.split(4)[0], arrange_1to12.split(4)[1], arrange_1to12.split(4)[2]\n",
    "\n",
    "print(magenta('original tensor\\n'), arrange_1to12)\n",
    "print(magenta('\\n3 partitions of tensor using.split:\\n'),arrange_0to3,'\\n' ,arrange_4to7,'\\n' ,arrange_8to11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[10;10;35m10 sentences, of 5 words, each with a 10 integer embedding\n",
      "\u001b[0m\n",
      "torch.Size([10, 5, 10])\n",
      "\u001b[10;10;35m\n",
      "Reshaping into 5 heads with a dimensionality of 2 instead of 10\n",
      "\u001b[0m\n",
      "torch.Size([10, 5, 5, 2])\n"
     ]
    }
   ],
   "source": [
    "# Experimenting with torch reshape\n",
    "\n",
    "num_examples = 10\n",
    "seq_length = 5\n",
    "embed_size = 10\n",
    "heads = 5\n",
    "\n",
    "x = torch.arange(num_examples*seq_length*embed_size).reshape(num_examples, seq_length, embed_size)\n",
    "print(magenta('10 sentences, of 5 words, each with a 10 integer embedding\\n'))\n",
    "print(x.shape)\n",
    "\n",
    "print(magenta('\\nReshaping into 5 heads with a dimensionality of 2 instead of 10\\n'))\n",
    "\n",
    "x = x.reshape(num_examples, seq_length, heads, embed_size // heads)\n",
    "print(x.shape) # Numbers are arranged in a weird way, but does that really matter on intialization? probably not"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
